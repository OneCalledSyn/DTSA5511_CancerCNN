{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Problem Statement**\n\nHistopathology is the study of diseased cells using a microscope. We will be using pictures taken by histopathologists of cancerous and non-cancerous lymph nodes cells, cleaning and preprocessing those images, and subsequently building a Convolutional Neural Network, or CNN, architecture in an attempt to perform binary classification of each photo as either metastatic cancer or a healthy tissue sample. Our first steps should be to read in the data and explore its structure and volume.","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are given a directory of images called test, a directory of images called train, a sample submission .csv, and a .csv of the labels for the training data. Let's start checking out what's inside each of them.","metadata":{}},{"cell_type":"code","source":"base_dir = '/kaggle/input/histopathologic-cancer-detection/'\ntrain_dir = base_dir + 'train'\ntest_dir = base_dir + 'test'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(base_dir + \"sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = pd.read_csv(base_dir + 'train_labels.csv')\nlabels.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the labels file contains just the id of the photo from the training set, and the class label of either a 0 for healthy cells or a 1 for metastatic cancer present, for a total size of 220025 rows of 2 columns.","metadata":{}},{"cell_type":"code","source":"f\"The ratio of regular pictures to cancer pictures is {round(labels['label'].value_counts()[0] / labels['label'].value_counts()[1], 2)}\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are about 50% more regular pictures than cancerous picture, which means the class split is about 60/40. Fortunately the classes are not ridiculously imbalanced to the point where we would see problems. Often times in datasets dealing with fraud or disease detection, the positive case is so rare that models will learn to simply guess the negative case every time to max out accuracy easily.","metadata":{}},{"cell_type":"code","source":"sns.set_theme(palette=\"pastel\")\nsns.countplot(labels, x = 'label')\nplt.xticks(range(2), ['Regular', 'Cancer'])\nplt.xlabel(\"Labels\")\nplt.ylabel(\"Count\")\nplt.title(\"Label Counts\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we can open up some of the images from the training set and pair them with their matching label from the labels file.","metadata":{}},{"cell_type":"code","source":"from PIL import Image\n\nfig = plt.figure(figsize = (8, 8))\ntrain_files = os.listdir(base_dir + 'train')\n\nfor i, pic in enumerate(train_files[0:25]):\n    ax = fig.add_subplot(5, 5, i + 1)\n    img = Image.open(base_dir + 'train/' + pic)\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    \n    label = labels.loc[labels[\"id\"] == pic.split('.')[0], 'label'].values[0]\n    ax.set_title(f\"{['Cancer' if label == 1 else 'Regular'][0]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While the images obviously look quite different, to the untrained human eye with no medical expertise, it would be quite difficult to differentiate which images present with cancerous cells. Hopefully our model is able to teach itself how to tell them apart much faster than humans learn to tell them apart!","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize = (8, 8))\ntest_files = os.listdir(base_dir + 'test')\n\nfor i, pic in enumerate(test_files[0:25]):\n    ax = fig.add_subplot(5, 5, i + 1)\n    img = Image.open(base_dir + 'test/' + pic)\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The test set images look fairly similar, with the obvious exception of the labels, which are unavailable for us to use.","metadata":{}},{"cell_type":"code","source":"sample = Image.open(base_dir + 'test/' + test_files[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The individual images are 96x96 pixels, for a total of 9216 pieces of information per picture. We would like to read in the training images with the powerful keras function flow_from_directory, but to do that we must first preprocess the file structure. Currently, all of the images of both classes are stored in the same directory, but we need to create subdirectories for each class or the keras function will be confused.","metadata":{}},{"cell_type":"code","source":"import shutil\n\nos.makedirs(os.path.join('train', 'regular'), exist_ok=True)\nos.makedirs(os.path.join('train', 'cancer'), exist_ok=True)\n\nfor index, row in labels.iterrows():\n    print(index)\n    image_id = row['id']\n    decision = row['label']\n    source_path = os.path.join('train', f'{image_id}.tif')\n    \n    if decision == 0:\n        destination_path = os.path.join('train', 'regular', f'{image_id}.tif')\n    else:\n        destination_path = os.path.join('train', 'cancer', f'{image_id}.tif')\n        \n    shutil.move(source_path, destination_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that our file system is rearranged properly, we can work on the preprocessing of the actual data and modeling of the CNN model architecture.","metadata":{}},{"cell_type":"markdown","source":"**Preprocessing**","metadata":{}},{"cell_type":"markdown","source":"When dealing with image data, we will typically want to do several steps prior to the training and learning stages to ensure the best possible results in a standardized procedure:\n\n* Convert the image data for each file into a 1-D list so that our CNN model can ingest them easily. \n* Normalize the pixel values to a decimal value between 0 and 1, typically by dividing the RGB values by 255\n* Artifically boost the size of the dataset by taking images and adding in variants of the original, with some transformation done such as flipping, rotating, shearing, etc.","metadata":{}},{"cell_type":"code","source":"import sklearn\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# Break up data into training and validation set\ntrain, val = train_test_split(labels, stratify = labels.label, test_size = 0.2)\nprint(len(train), len(val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define hyperparameters + parameters\nimg_sz = (96, 96)\nclasses = 2\nbatch_sz = 32\nepochs = 20\nlearning_rate = 0.005","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize both the train and test data\ntrain_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20,\n    # Add in preprocessing transformations                               \n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    shear_range = 0.2,\n    zoom_range = 0.2,\n    horizontal_flip = True,\n    fill_mode = 'nearest')\n\ntest_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20,\n    # Add in preprocessing transformations\n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    shear_range = 0.2,\n    zoom_range = 0.2,\n    horizontal_flip = True,\n    fill_mode = 'nearest')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pull the data from each subdirectory into the ImageDataGenerator function\ntrain_generator = train_datagen.flow_from_directory(\n    '/kaggle/input/histopathologic-cancer-detection/',\n    classes = ['cancer', 'regular'],\n    color_mode = 'rgb',\n    target_size = img_size,\n    batch_size = batch_size,\n    class_mode = 'binary',\n    shuffle = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Modeling**","metadata":{}},{"cell_type":"markdown","source":"Our CNN has four convolutional layers, three max pooling layers, and two dense layers. The first convolutional layer has 32 filters with a kernel size of 3x3 and a ReLU activation function. The input shape is height and width of the images (96x96) and 3 channels for RGB. Then our first max pooling layer has a pool size of 2x2, which feeds into the second convolutional layer of 64 filters, a 3x3 kernel, and uses a ReLU activation function. The second max pooling layer is identical to the first max pooling layer. The third convolutional layer has 128 filters, a kernel size of 3x3, and employs a ReLU activation function. Again, the third max pooling layer is identical to the first two. The Flatten layer squishes the output of the previous layer into a 1-D vector. Next, a fully connected or dense layer of 512 units and a ReLU activation function feeds into a Dropout layer, which is a regularization technique that kills off a portion of the neurons at random during training, helping to prevent overfitting. The final layer is a dense layer with a single unit with a sigmoid activation function, which outputs a value in the range (0, 1) inclusive. Sigmoid is perfect for binary classification problems as we can map the output to one class if the output is over 0.5 and the other class if not.","metadata":{}},{"cell_type":"code","source":"model1 = Sequential()\n\nmodel1.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 3)))\nmodel1.add(MaxPooling2D((2, 2)))\n\nmodel1.add(Conv2D(64, (3, 3), activation='relu'))\nmodel1.add(MaxPooling2D((2, 2)))\n\nmodel1.add(Conv2D(128, (3, 3), activation='relu'))\nmodel1.add(MaxPooling2D((2, 2)))\n\nmodel1.add(Flatten())\n\nmodel1.add(Dense(512, activation='relu'))\nmodel1.add(Dropout(0.5))\n\nmodel1.add(Dense(1, activation='sigmoid'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use Adam as our optimizer, binary crossentropy as our loss function, and accuracy as our standard evaluation metric, as well as AUC as an additional evaluation metric, since the final output submitted to the Kaggle competition will be judged on AUC.\n\nAdam, or Adaptive Moment Estimation, is an algorithm that builds off of stochastic gradient descent (SGD) by adjusting the learning rate over time using the first and second moments of the gradient. Regular SGD can suffer from slow convergence or even divergence with a poorly chosen learning rate that remains fixed through the learning process. Adam's adaptive learning rate speeds up convergence time and reduces the chances of divergence as well. Adam was released to the general public in 2014 by Diederik Kingma, writing \"The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters.\" in Kingma et al., 2014. \n\nBCE loss measures the difference between true labels and the predicted pseudoprobabilities output by the final sigmoid layer in the CNN model. The loss is minimized when the predicted pseduoprobabilities are closest to the true labels. Therefore, BCE loss is commonly used in binary classification problems to train models with gradient descent optimization algorithms, such as Adam.\n\nAUC, or \"Area Under the ROC Curve\" is a metric used to evaluate the performance of a binary classification model, although usually for datasets with imbalanced class sizes. Since our dataset has balanced classes, it is not particularly more useful than other available metrics like accuracy, but again it is part of the competition so we should include it to have a good yardstick of our leaderboard performance.","metadata":{}},{"cell_type":"code","source":"model1.compile(optimizer='Adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy', Metrics.AUC()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model1.fit_generator(\n    train_generator,\n    steps_per_epoch = train_generator.n//batch_size,\n    epochs = num_epochs,\n    validation_data = test_generator,\n    validation_steps = test_generator.n//batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['AUC'])\nplt.plot(history.history['val_AUC'])\nplt.title('Model AUC')\nplt.ylabel('AUC')\nplt.xlabel('Epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, let's do some hyperparameter tuning using our old friend, grid search.","metadata":{}},{"cell_type":"code","source":"from keras.optimizers import Adam\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n# Define values to search\nbatch_sizes = [16, 32, 64]\nepochs = [20, 30, 40]\nlearning_rates = [0.001, 0.003, 0.005]\n\n# Define the model architecture\ndef RNN_model(learning_rate=0.005):\n    model1.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 3)))\n    model1.add(MaxPooling2D((2, 2)))\n    model1.add(Conv2D(64, (3, 3), activation='relu'))\n    model1.add(MaxPooling2D((2, 2)))\n    model1.add(Conv2D(128, (3, 3), activation='relu'))\n    model1.add(MaxPooling2D((2, 2)))\n    model1.add(Flatten())\n    model1.add(Dense(512, activation='relu'))\n    model1.add(Dropout(0.5))\n    model1.add(Dense(1, activation='sigmoid'))\n    \n    optimizer = Adam(lr=learning_rate)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n\n# Create a KerasClassifier object to perform looping\nmodel = KerasClassifier(build_fn=RNN_model, epochs=20, batch_size=32, verbose=0)\nparam_grid = dict(batch_size=batch_sizes, epochs=epochs, learning_rate=learning_rates)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, verbose=1)\ngrid_result = grid.fit(X_train, y_train)\n\n# Print the results\nprint(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our best hyperparameter combination was a batch size of 32, over 40 training epochs, with a learning rate of 0.003, yielding an accuracy of ~ 87%.","metadata":{}},{"cell_type":"markdown","source":"**Model 2**\n\nLet's see if we can get any better results using a simpler architecture, with far fewer layers, by cutting down to one convolutional layer and one max pooling layer. Remember, a more parsimonious model is always preferred to a more complex model given that they have the same explanatory power.","metadata":{}},{"cell_type":"code","source":"model2 = Sequential()\n\nmodel2.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 3)))\nmodel2.add(MaxPooling2D((2, 2)))\n\nmodel2.add(Flatten())\n\nmodel2.add(Dense(512, activation='relu'))\nmodel2.add(Dropout(0.5))\n\nmodel2.add(Dense(1, activation='sigmoid'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define values to search\nbatch_sizes = [16, 32, 64]\nepochs = [20, 30, 40]\nlearning_rates = [0.001, 0.003, 0.005]\n\n# Define the model architecture\ndef RNN_model2(learning_rate=0.005):\n    model2 = Sequential()\n    model2.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 3)))\n    model2.add(MaxPooling2D((2, 2)))\n    model2.add(Flatten())\n    model2.add(Dense(512, activation='relu'))\n    model2.add(Dropout(0.5))\n    model2.add(Dense(1, activation='sigmoid'))\n    \n    optimizer = Adam(lr=learning_rate)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n\n# Create a KerasClassifier object to perform looping\nmodel2 = KerasClassifier(build_fn=RNN_model2, epochs=20, batch_size=32, verbose=0)\nparam_grid2 = dict(batch_size=batch_sizes, epochs=epochs, learning_rate=learning_rates)\ngrid2 = GridSearchCV(estimator=model, param_grid=param_grid2, cv=3, verbose=1)\ngrid_result2 = grid2.fit(X_train, y_train)\n\n# Print the results\nprint(f\"Best: {grid_result2.best_score_} using {grid_result2.best_params_}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Results + Analysis + Conclusion**\n\nOur best hyperparameter combination was a batch size of 32, over 40 training epochs, with a learning rate of 0.005, yielding an accuracy of ~ 82%. We can see that while our results are still fairly good, we had better performance using the more complex model that was able to learn the separation slightly better. This makes sense because the more convolutional layers we have in the neural network, the more complex shapes and patterns they are able to learn as features. Histopathology images of cancer cells can be very difficult to parse as humans and may need several layers of convolution for sufficient deep learning. Higher amounts of epochs and slower learning rates were especially critical in model performance being as good as it was. With lower numbers of epochs the CNN lacked sufficient time to learn correctly and with a higher learning rate the model may fail to converge and overshoot the global minimum in the gradient descent.\n\nCNN was an apt choice as an overarching deep learning technique, as it was specifically created to deal with classification of images, which is exactly the task set before us in this cancer cell detection challenge. To recap, our best model had four convolutional layers, three max pooling layers, and two dense layers. The first convolutional layer has 32 filters with a kernel size of 3x3 and a ReLU activation function. The input shape is height and width of the images (96x96) and 3 channels for RGB. Then our first max pooling layer has a pool size of 2x2, which feeds into the second convolutional layer of 64 filters, a 3x3 kernel, and uses a ReLU activation function. The second max pooling layer is identical to the first max pooling layer. The third convolutional layer has 128 filters, a kernel size of 3x3, and employs a ReLU activation function. Again, the third max pooling layer is identical to the first two. The Flatten layer squishes the output of the previous layer into a 1-D vector. Next, a fully connected or dense layer of 512 units and a ReLU activation function feeds into a Dropout layer, which is a regularization technique that kills off a portion of the neurons at random during training, helping to prevent overfitting. The final layer is a dense layer with a single unit with a sigmoid activation function, which outputs a value in the range (0, 1) inclusive. Sigmoid is perfect for binary classification problems as we can map the output to one class if the output is over 0.5 and the other class if not.\n\nWe learned that more complicated imagery like Gram stained cell histopathology microscope scans may require a more complex CNN than some of the more simplistic CNN tasks such as looking at faces or cats vs dogs, where there are a few main concepts to learn that are fairly obvious, such as eyes, mouth, nose, and tail. Cancerous cells can take on many different forms and the variations in cells can be amazingly complex even compared to the variations in human faces or animals. Our deeper CNN model managed to outperform our simpler model by a healthy margin due to its additional convolutional layers.\n\nPotential enhancements for future endeavors could be adding in another regularization technique on top of dropout, which is weight decay, to prevent overfitting. We could try to implement transfer learning, where a pretrained model is used as a launching point for us to build on top of. Some examples of this are ResNet101, MobileNet, and Xception which are available to use from the Keras pretrained models. We could also build multiple CNN models and combine their outputs to form an ensemble learning network, which could also help reduce overfitting and generalize the performance of the model. Last but not least, we could always add more layers into our highest performing CNN and make the model even deeper, to see if the performs becomes any stronger.","metadata":{}}]}